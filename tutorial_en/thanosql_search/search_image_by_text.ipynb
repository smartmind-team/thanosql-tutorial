{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad7df84",
   "metadata": {},
   "source": [
    "# __Search image by text__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01de980",
   "metadata": {},
   "source": [
    "## Preface\n",
    "\n",
    "- Tutorial Difficulty : â˜…â˜…â˜†â˜†â˜†\n",
    "- 7 min read\n",
    "- Languages : [SQL](https://en.wikipedia.org/wiki/SQL) (100%)\n",
    "- File location : tutorial_en/thanosql_search/search_image_by_text.ipynb\n",
    "- References : [Unsplash Dataset - Lite](https://unsplash.com/data), [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e19c02",
   "metadata": {},
   "source": [
    "## Tutorial Introduction\n",
    "\n",
    "<div class=\"admonition note\">\n",
    "    <h4 class=\"admonition-title\">Understanding Text Digitization Techniques</h4>\n",
    "    <p>For computers to understand natural language, natural language must be quantified. Recently, studies on pre-learning models such as <a href=\"https://en.wikipedia.org/wiki/BERT_(language_model)\">BERT</a> and <a href=\"https://en.wikipedia.org/wiki/GPT-3\">GPT-3</a> have been actively carried out, showing remarkable results. These models identify the meaning of each sentence based on <a href=\"https://en.wikipedia.org/wiki/Self-supervised_learning\">Self-Supervised Learning</a>, and numerically express each sentence with a similar meaning in a low-dimensional space so that they are closely located. It supports learning without labeling by determining whether each sentence/context is true/false by randomly shuffling the order between sentences or masking some words.</p>\n",
    "</div>\n",
    "\n",
    "The problem of handling different forms of input together, such as text and images, is called multi-modal. **\"CLIP: Connecting Text and Image\"** deals with the understanding of low-dimensional space quantified with a representative multi-modal model. If the existing model learned only the <a href=\"https://en.wikipedia.org/wiki/Feature_(machine_learning)\">features</a> of the image itself, the multi-modal model can use both images and text as input data while simultaneously learning the features of the text describing the image. In addition, by placing text and images together in a low-dimensional space, it is possible to judge the similarity between text and images, and by applying this, it can be used as a search algorithm.\n",
    "\n",
    "ThanoSQL uses artificial intelligence algorithms to quantify datasets. The digitized data is stored in the DB column, and is used to search for similar images through digitization results and similarity calculations of the input text.\n",
    "\n",
    "__The following is an example and application of ThanoSQL text-image search algorithm.__\n",
    "\n",
    "- Describe the desired scene in text in the image or video you have and search for the image that is most similar to it. Hear text-based, rather than keywords, descriptions of the products users are searching for and expose the most similar product images.\n",
    "- Search for the time you want to place the advertisement you want in YouTube videos, etc. In order to place travel advertisements, you can easily search for scenes with mountains or camping scenes and insert advertisements.\n",
    "\n",
    "<div class=\"admonition note\">\n",
    "    <h4 class=\"admonition-title\">In this tutorial</h4>\n",
    "    <p>ðŸ‘‰ Unsplash released images of more than 200,000 photographers for free as a dataset for AI. <code>Unsplash Dataset - Lite</code> consists of 25,000 nature-themed images and comes with 25,000 keywords. </p>\n",
    "</div>\n",
    "\n",
    "In this tutorial, we will use the text-image search model to search for the desired image in text from 25,000 images in the `Unsplash Dataset - Lite` dataset in the ThanoSQL DB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65fedaf-1054-45fa-84ab-de45bc457413",
   "metadata": {},
   "source": [
    "## __0. Prepare Dataset and Model__\n",
    "\n",
    "To use the query syntax of ThanoSQL, you must create an API token and run the query below, as mentioned in the [ThanoSQL Workspace](https://docs.thanosql.ai/en/getting_started/how_to_use_ThanoSQL/#5-thanosql-workspace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e39f0-21fc-4df8-b3bb-caeb4749e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext thanosql\n",
    "%thanosql API_TOKEN=<Issued_API_TOKEN>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073a6182",
   "metadata": {},
   "source": [
    "### __Prepare Dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd073cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "GET THANOSQL DATASET unsplash_data\n",
    "OPTIONS (overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec2d024",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\">\n",
    "    <h4 class=\"admonition-title\">Query Details</h4>\n",
    "    <ul>\n",
    "        <li>\"<strong>GET THANOSQL DATASET</strong>\" Use the query syntax to save the desired dataset to the workspace. </li>\n",
    "        <li>\"<strong>OPTIONS</strong>\" Specifies the option to use for <strong>GET THANOSQL DATASET</strong> via query syntax.\n",
    "        <ul>\n",
    "            <li>\"overwrite\" : Set whether to overwrite if a dataset with the same name exists. If True, the old dataset is replaced with the new dataset (True|False, DEFAULT : False) </li>\n",
    "        </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade49a9f-600a-4f67-8a9a-52ca297b212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "COPY unsplash_data \n",
    "OPTIONS (overwrite=True)\n",
    "FROM 'thanosql-dataset/unsplash_data/unsplash.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6845948",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\">\n",
    "    <h4 class=\"admonition-title\">Query Details</h4>\n",
    "    <ul>\n",
    "        <li>Use the \"<strong>COPY</strong>\" query syntax to specify the dataset name to store in the DB. </li>\n",
    "        <li>\"<strong>OPTIONS</strong>\" Specifies the options to use for <strong>COPY</strong> query syntax.\n",
    "        <ul>\n",
    "            <li>\"overwrite\" : Set whether or not a dataset with the same name can be overwritten if it exists on the DB. If True, the existing dataset is changed to the new dataset (True|False, DEFAULT: False) </li>\n",
    "        </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f797e8",
   "metadata": {},
   "source": [
    "### __Prepare the Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f326b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "GET THANOSQL MODEL tutorial_search_clip\n",
    "OPTIONS (overwrite=True)\n",
    "AS tutorial_search_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38bd64f",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\">\n",
    "    <h4 class=\"admonition-title\">Query Details </h4>\n",
    "    <ul>\n",
    "        <li>\"<strong>GET THANOSQL MODEL</strong>\" Use the query syntax to store the desired model in the workspace and DB. </li>\n",
    "        <li>\"<strong>OPTIONS</strong>\" Use the query syntax to specify the options to use for <strong>GET THANOSQL MODEL</strong>.\n",
    "        <ul>\n",
    "            <li>\"overwrite\" : Set whether datasets with the same name can be overwritten if they exist. If True, the existing dataset is changed to a new dataset (True|False, DEFAULT: False) </li>\n",
    "        </ul>\n",
    "        </li>\n",
    "        <li>Use the query syntax \"<strong>AS</strong>\" to name the model. If you are not using the AS syntax, accept the name of <code>THANOSQL MODEL</code>.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452bf799-d5dc-4a41-9405-cff7a5023189",
   "metadata": {},
   "source": [
    "## __1. Check Dataset__\n",
    "\n",
    "To create a text-image search model, we use the `unsplash_data` table stored in ThanoSQL DB. Execute the query statement below and check the contents of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b841e-61e1-4c15-aecf-f99435bc6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "SELECT photo_id, image_path, photo_image_url, photo_description, ai_description\n",
    "FROM unsplash_data\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460bdb35",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\">\n",
    "    <h4 class=\"admonition-title\">Understanding Data</h4>\n",
    "    <ul>\n",
    "        <li><code>photo_id</code> Unique id column name of image </li>\n",
    "        <li><code>image_path</code> Column name of the path where the image is located </li>\n",
    "        <li><code>photo_image_url</code> Column name indicating the address of the original image in the website unsplash </li>\n",
    "        <li><code>photo_description</code> Column name that represents a short human description of the image.</li>\n",
    "        <li><code>ai_description</code> Column name that describes the image generated by AI</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db35b04f-7fce-4da3-b4d0-021d19b5427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "PRINT IMAGE \n",
    "AS\n",
    "SELECT image_path \n",
    "FROM unsplash_data \n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5094f708-953c-454e-b8b0-961fa50cd979",
   "metadata": {},
   "source": [
    "## __2. Creating an Image Numerical Model for Text Search__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5963aa2e",
   "metadata": {},
   "source": [
    "<div class=\"admonition danger\">\n",
    "    <h4 class=\"admonition-title\">Notes</h4>\n",
    "    <p>Because text-image retrieval algorithms take a long time to learn and use pre-trained models with a total of 400 million datasets, we omit the learning process using the \"<strong>BUILD MODEL</strong>\" query syntax in this tutorial. The <code>tutorial_search_clip</code> model is used as a base algorithm by importing a pre-trained model using <code>clipen</code>. When the \"<strong>CONVERT USING</strong>\" query statement is executed, a column in which the image is digitized with \"model name (<code>tutorial_search_clip</code>)_base algorithm name (<code>clipen</code>)\" is automatically created, and \"<strong>SEARCH IMAGE</strong>\" When the query statement is executed, an image similarity column is automatically created with \"model name (<code>tutorial_search_clip</code>)_base algorithm name (<code>clipen</code>)_similarity number(1)\". \"Number\" here means the number of texts used in the search. When a search is performed with two or more texts, the number of columns is sequentially increased according to the order. For more details, see below.</p>\n",
    "</div>\n",
    "(Expected time to execute query: 3 min)  \n",
    "\n",
    "<p>Run the following query syntax \"<strong>CONVERT USING</strong>\" to quantify the <code>unsplash_data</code> images. Numerical results are stored in the new <mark style=\"background-color:#D7D0FF\">tutorial_search_clip_clipen</mark> column. (The resulting column name will be added as {model_name}_{base_model_name}) </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748d8ef-0a7c-4e0f-96a1-8b63db98f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "CONVERT USING tutorial_search_clip\n",
    "OPTIONS (\n",
    "    image_col=\"image_path\", \n",
    "    table_name=\"unsplash_data\", \n",
    "    batch_size=128\n",
    "    )\n",
    "AS \n",
    "SELECT *\n",
    "FROM unsplash_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb189a3",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\">\n",
    "    <h4 class=\"admonition-title\">Query Details</h4>\n",
    "    <ul>\n",
    "        <li>The query syntax \"<strong>CONVERT USING</strong>\" uses the <code>tutorial_search_clip</code> model as an algorithm for image quantification.</li>\n",
    "        <li>The query syntax \"<strong>OPTIONS</strong>\" defines the variables required for image quantification.\n",
    "        <ul>\n",
    "            <li>\"table_name\" : table name to be stored in ThanoSQL DB</li>\n",
    "            <li>\"image_col\" : Column name containing the image path </li>\n",
    "            <li>\"batch_size\" : The size of the dataset bundle read in one training. According to the paper, the larger the number, the better the learning performance, but considering the size of the memory, 128 is used. (DEFAULT: 16) </li>\n",
    "        </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98017f89-df5c-4cea-926e-720c95473d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "SELECT *\n",
    "FROM unsplash_data\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b1923-1f24-470e-8221-3ba7b4d1a52c",
   "metadata": {},
   "source": [
    "## __3. Search for images by text__\n",
    "\n",
    "Perform a text-based image search using the \"__SEARCH IMAGE__\" query syntax and the `tutorial_search_clip` model you created.\n",
    " You can. Execute the following query syntax with the text \"a black cat\" and embedded `unsplash_data`\n",
    "Calculate the similarity of images. The result value is in the newly added <mark style=\"background-color:#D7D0FF \">tutorial_search_clip_clipen_similarity1</mark> column.\n",
    "is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc17382-5e4f-4c60-8cf2-f56f2d9f9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "SEARCH IMAGE text=\"a black cat\"\n",
    "USING tutorial_search_clip\n",
    "AS \n",
    "SELECT * \n",
    "FROM unsplash_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e614d40",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\">\n",
    "    <h4 class=\"admonition-title\">Query Details</h4>\n",
    "    <ul>\n",
    "        <li>Specifies that images will be found using the query syntax \"<strong>SEARCH IMAGE</strong>\". Enter the text content of the image you want to find using the \"text\" variable. </li>\n",
    "        <li>The query syntax \"<strong>USING</strong>\" specifies to use <code>tutorial_search_clip</code> as the model to use for the search.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2009d0",
   "metadata": {},
   "source": [
    "Execute the query syntax below to determine the similarity of the 5 images most similar to the text 'a black cat'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29188e1-4647-4402-8b77-0d73afee24cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "SELECT image_path, tutorial_search_clip_clipen_similarity1 \n",
    "FROM (\n",
    "    SEARCH IMAGE text=\"a black cat\"\n",
    "    USING tutorial_search_clip\n",
    "    AS \n",
    "    SELECT * \n",
    "    FROM unsplash_data\n",
    "    )\n",
    "ORDER BY tutorial_search_clip_clipen_similarity1 DESC \n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a32a0a",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\">\n",
    "    <h4 class=\"admonition-title\">Query Details</h4>\n",
    "    <ul>\n",
    "        <li>The query syntax \"<strong>SEARCH IMAGE</strong>\" calculates and returns the similarity between the text entered and the image.</li>\n",
    "        <li>The first \"<strong>SELECT</strong>\" query syntax selects the <mark style=\"background-color:#D7D0FF \">image_path</mark> column and the <mark style=\"background-color:#D7D0FF \">tutorial_search_clip_clipen_similarity1</mark> column from the query result in parentheses.</li>\n",
    "        <li>The \"<strong>ORDER BY</strong>\" query syntax sorts the results based on the value of the <mark style=\"background-color:#D7D0FF \">tutorial_search_clip_clipen_similarity1</mark> column, in descending order (\"<strong>DESC</strong>\"), of which the top 5 It prints the result of (\"<strong>LIMIT</strong>\" 5).</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5449f9ca",
   "metadata": {},
   "source": [
    "By applying the previous query syntax with the \"__PRINT__\" statement, you can immediately check the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f907e-b8f9-48a7-82a0-efdd4ef4b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "PRINT IMAGE \n",
    "AS (\n",
    "    SELECT image_path, tutorial_search_clip_clipen_similarity1 \n",
    "    FROM (\n",
    "        SEARCH IMAGE text=\"a black cat\"\n",
    "        USING tutorial_search_clip\n",
    "        AS \n",
    "        SELECT * \n",
    "        FROM unsplash_data\n",
    "        )\n",
    "    ORDER BY tutorial_search_clip_clipen_similarity1 DESC \n",
    "    LIMIT 5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefd258",
   "metadata": {},
   "source": [
    "<div class=\"admonition note\">\n",
    "    <h4 class=\"admonition-title\">Query Details</h4>\n",
    "    <p>This query, combined with the query above, consists of three steps.</p>\n",
    "    <ul>\n",
    "        <li>The query syntax \"<strong>SELECT</strong>\" in the first parentheses produces the result of the step immediately above.</li>\n",
    "        <li>Use the \"<strong>PRINT IMAGE</strong>\" query syntax to print that image.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5170393c-928a-49ae-b3c1-fe6782ab2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "PRINT IMAGE \n",
    "AS (\n",
    "    SELECT image_path, tutorial_search_clip_clipen_similarity1 \n",
    "    FROM (\n",
    "        SEARCH IMAGE text=\"a dog on a chair\"\n",
    "        USING tutorial_search_clip\n",
    "        AS \n",
    "        SELECT * \n",
    "        FROM unsplash_data\n",
    "        )\n",
    "    ORDER BY tutorial_search_clip_clipen_similarity1 DESC \n",
    "    LIMIT 5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d325ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "PRINT IMAGE \n",
    "AS (\n",
    "    SELECT image_path, tutorial_search_clip_clipen_similarity1 \n",
    "    FROM (\n",
    "        SEARCH IMAGE text=\"gloomy photos\"\n",
    "        USING tutorial_search_clip\n",
    "        AS \n",
    "        SELECT * \n",
    "        FROM unsplash_data\n",
    "        )\n",
    "    ORDER BY tutorial_search_clip_clipen_similarity1 DESC \n",
    "    LIMIT 5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc8f19-ab81-496e-860b-dda32b69c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "PRINT IMAGE \n",
    "AS (\n",
    "    SELECT image_path, tutorial_search_clip_clipen_similarity1 \n",
    "    FROM (\n",
    "        SEARCH IMAGE text=\"the feeling when your program finally works\"\n",
    "        USING tutorial_search_clip\n",
    "        AS \n",
    "        SELECT * \n",
    "        FROM unsplash_data\n",
    "        )\n",
    "    ORDER BY tutorial_search_clip_clipen_similarity1 DESC \n",
    "    LIMIT 5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389cb4a",
   "metadata": {},
   "source": [
    "## __4. In Conclusion__\n",
    "\n",
    "In this tutorial, we used a multimodal text/image quantification model to search for images via text in `unsplash dataset`. As it is a beginner's tutorial, we focused on getting visible results through simple queries. If you use image search with a slightly more colorful query, you will get a value closer to the desired result.\n",
    "\n",
    "<div class=\"admonition tip\">\n",
    "    <h4 class=\"admonition-title\">Inquiries about deploying a model for your own service</h4>\n",
    "    <p>If you have any difficulties in creating your own model using ThanoSQL or applying it to the service, please feel free to contact us belowðŸ˜Š</p>\n",
    "    <p>For inquiries about building a text-image search model: contact@smartmind.team</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
