{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad7df84",
   "metadata": {},
   "source": [
    "# __Search image by text__ \n",
    "\n",
    "**Understand text digitization techniques**\n",
    "---\n",
    "ðŸ‘‰ Natural languages must be quantified for computers to understand them. Recently, research on pre-learning models such as BERT and GPT-3 has been actively conducted and has shown remarkable results. Based on self-supervised learning, these models grasp the meaning of each sentence and represent each sentence with a similar meaning in a low-dimensional space in close proximity. By randomly mixing the order between sentences or masking some words to determine whether each sentence/context is true or false, it supports learning without labeling.\n",
    "\n",
    "**In this tutorial**\n",
    "---\n",
    "ðŸ‘‰ Unsplash released images of more than 200,000 photographers for free as a dataset for AI. Unsplash Dataset - Lite consists of 25,000 nature-themed images, and comes with 25,000 keywords.\n",
    "\n",
    "In this tutorial, we will use the text-image search model to search for the desired image in text from 25,000 images in the Unsplash Dataset - Lite dataset in ThanoSQL DB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65fedaf-1054-45fa-84ab-de45bc457413",
   "metadata": {},
   "source": [
    "## __0. Preparing a dataset__\n",
    "\n",
    "To use the query syntax of ThanoSQL, you must create an API token and run the query below, as mentioned in the [ThanoSQL Workspace](https://docs.thanosql.ai/quick_start/how_to_use_ThanoSQL/#5-thanosql)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e39f0-21fc-4df8-b3bb-caeb4749e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext thanosql\n",
    "%thanosql API_TOKEN=<Issued_API_TOKEN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade49a9f-600a-4f67-8a9a-52ca297b212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "COPY unsplash_data \n",
    "OPTIONS(overwrite = True)\n",
    "FROM 'tutorial_data/unsplash_data/unsplash.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67fae5",
   "metadata": {},
   "source": [
    "__OPTIONS__ : \n",
    "\n",
    "When __overwrite is true__, the user can create a data table with the same name as the previously created data table.  \n",
    "On the other hand, when __overwrite is False__, the user cannot create a data table with the same name as the previously created data table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452bf799-d5dc-4a41-9405-cff7a5023189",
   "metadata": {},
   "source": [
    "## __1. Check dataset__\n",
    "\n",
    "To create a text-image search model, we use the unsplash_data table stored in the ThanoSQL DB. Run the query statement below and check the contents of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b841e-61e1-4c15-aecf-f99435bc6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "SELECT photo_id, image_path, photo_image_url, photo_description, ai_description\n",
    "FROM unsplash_data\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de468d85",
   "metadata": {},
   "source": [
    "__Understanding Data__\n",
    "- `photo_id` Unique id column name for image\n",
    "- `image_path` Column name of the path where the image is located\n",
    "- `photo_image_url` Column name representing the source image address in the website unsplash\n",
    "- `photo_description` The name of the column that represents a short human-generated description of the image\n",
    "- `ai_description` Column name that represents the description of the image created by AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db35b04f-7fce-4da3-b4d0-021d19b5427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "PRINT IMAGE \n",
    "AS\n",
    "SELECT image_path \n",
    "FROM unsplash_data \n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5094f708-953c-454e-b8b0-961fa50cd979",
   "metadata": {},
   "source": [
    "## __2. Create an image quantization model for text retrieval__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5963aa2e",
   "metadata": {},
   "source": [
    "__Note__\n",
    "\n",
    "Because the text-image search algorithm takes a long time to learn and uses a pre-trained model with a total of 400 million datasets, the learning process using the \"BUILD MODEL\" query syntax is omitted in this tutorial. The tutorial_search_clip model is a base algorithm that uses a pre-learned model using clippen. Running the \"CONVERT USING\" query syntax automatically creates columns that are digitized by the \"tutorial_search_clip_base algorithm name (clipen)\", and running the \"SEARCH IMAGE\" query syntax automatically creates columns similar to the \"tutorial_search_clip\"_base image (1 pen) color. The excitation number \"number\" refers to the number of text used in the search. If a search is performed with more than one text, the number of columns is increased sequentially according to the order. Please refer to the details below for details below.\n",
    "\n",
    "(Expected time required for query execution: 3 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748d8ef-0a7c-4e0f-96a1-8b63db98f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "CONVERT USING tutorial_search_clip\n",
    "OPTIONS (\n",
    "    image_col=\"image_path\", \n",
    "    table_name=\"unsplash_data\", \n",
    "    batch_size=128\n",
    "    )\n",
    "AS \n",
    "SELECT *\n",
    "FROM unsplash_data LIMIT 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5157c654",
   "metadata": {},
   "source": [
    "__Query Details__ \n",
    "\n",
    "- The query syntax \"__CONVERT USING__\" uses the 'tutorial_search_clip' model as an algorithm for image quantification.  \n",
    "- The \"__OPTIONS__\" query syntax defines the variables required for image quantification. Defines the table name (\"table_name\") to be stored within the ThanoSQL DB. Define the column name where you saved the image's storage path in \"image_col\". Use 'image_path' in this tutorial. \"batch_size\" is the size of a bundle of datasets read in one learning. According to the paper, the larger the learning performance increases, but we use 128 considering the size of the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98017f89-df5c-4cea-926e-720c95473d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "SELECT *\n",
    "FROM unsplash_data\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0eeb1",
   "metadata": {},
   "source": [
    "You can see that the column `tutorial_search_clip_clip` was created as a result of the `CONVERT` query. A column containing the embedding value will be added with the name `{model_name}_{base_model_name}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b1923-1f24-470e-8221-3ba7b4d1a52c",
   "metadata": {},
   "source": [
    "## ___3. Search for images in text__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f14295",
   "metadata": {},
   "source": [
    "You can search for images using the query syntax \"__SEARCH IMAGE__\" and the image quantization model you created (`tutorial_search_clip`). First, search in one text and see the results. \"Model name (`tutorial_search_clip`)_base algorithm name (`clip`)_similarity number (1))\" confirms that the image similarity column is automatically generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc17382-5e4f-4c60-8cf2-f56f2d9f9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "SEARCH IMAGE text=\"a black cat\"\n",
    "USING tutorial_search_clip\n",
    "AS \n",
    "SELECT * \n",
    "FROM unsplash_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b8d781",
   "metadata": {},
   "source": [
    "__Query Details__ \n",
    "- State that you will find the image using the query syntax \"__SEARCH IMAGE__\". Use the \"text\" variable to enter the text content of the image you want to find. \n",
    "- The query syntax \"__USING__\" specifies the use of 'tutorial_search_clip' as the model to be used for the search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2009d0",
   "metadata": {},
   "source": [
    "You can see that the row `tutorial_search_clip_clip_similarity1` was created as a result of the query syntax. To use as a search algorithm, you need to select and view the most similar images using similarity calculations. Perform the query syntax below to see the five images that are most similar to the text in the DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29188e1-4647-4402-8b77-0d73afee24cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "SEARCH IMAGE text=\"a black cat\"\n",
    "USING tutorial_search_clip\n",
    "AS \n",
    "SELECT * \n",
    "FROM unsplash_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeabe68",
   "metadata": {},
   "source": [
    "__Query Details__ \n",
    "- State that you will find the image using the query syntax \"__SEARCH IMAGE__\". Use the \"text\" variable to enter the text content of the image you want to find. \n",
    "- The query syntax \"__USING__\" specifies the use of `tutorial_search_clip` as the model to be used for the search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4010c2",
   "metadata": {},
   "source": [
    "You can see that the row `tutorial_search_clip_clip_similarity1` was created as a result of the query syntax. To use as a search algorithm, you need to select and view the most similar images using similarity calculations. Perform the query syntax below to see the five images that are most similar to the text in the DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddbaa9-e057-4f9d-89e7-73c74d684c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "SELECT image_path, tutorial_search_clip_clipen_similarity1 \n",
    "FROM (\n",
    "    SEARCH IMAGE text=\"a black cat\"\n",
    "    USING tutorial_search_clip\n",
    "    AS \n",
    "    SELECT * \n",
    "    FROM unsplash_data\n",
    "    )\n",
    "ORDER BY tutorial_search_clip_clipen_similarity1 DESC \n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0053a5",
   "metadata": {},
   "source": [
    "__Query Details__\n",
    "\n",
    "- The query syntax \"__SEARCH IMAGE__\" calculates and returns the similarity between the text and the image you entered.\n",
    "- The first \"__SELECT__\" query syntax selects the \"filepath\" column and the `tutorial_search_clip_clip_similarity1` column from the query results in parentheses. At this time, the `filepath` column is changed to the name of the column `image`.\n",
    "- The \"_ORDER BY__\" query syntax sorts the results by the value in the column `tutorial_search_clip_clip_similarity1`, which is in descending order (\"__DESC__\", and outputs the results of the top five (\"__LIMIT_\" 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e3263",
   "metadata": {},
   "source": [
    "The images most similar to the text 'a black cat' you entered are now displayed in order. If you use this query syntax with the statement \"__PRINT__\", you can see the result image immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f907e-b8f9-48a7-82a0-efdd4ef4b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "PRINT IMAGE \n",
    "AS (\n",
    "    SELECT image_path \n",
    "    AS image_path, tutorial_search_clip_clipen_similarity1 \n",
    "    FROM (\n",
    "        SEARCH IMAGE text=\"a black cat\"\n",
    "        USING tutorial_search_clip\n",
    "        AS \n",
    "        SELECT * \n",
    "        FROM unsplash_data\n",
    "        )\n",
    "    ORDER BY tutorial_search_clip_clipen_similarity1 DESC \n",
    "    LIMIT 5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5170393c-928a-49ae-b3c1-fe6782ab2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "PRINT IMAGE \n",
    "AS (\n",
    "    SELECT image_path, tutorial_search_clip_clipen_similarity1 \n",
    "    FROM (\n",
    "        SEARCH IMAGE text=\"a dog on a chair\"\n",
    "        USING tutorial_search_clip\n",
    "        AS \n",
    "        SELECT * \n",
    "        FROM unsplash_data\n",
    "        )\n",
    "    ORDER BY tutorial_search_clip_clipen_similarity1 DESC \n",
    "    LIMIT 5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d325ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "PRINT IMAGE \n",
    "AS (\n",
    "    SELECT image_path, tutorial_search_clip_clipen_similarity1 \n",
    "    FROM (\n",
    "        SEARCH IMAGE text=\"gloomy photos\"\n",
    "        USING tutorial_search_clip\n",
    "        AS \n",
    "        SELECT * \n",
    "        FROM unsplash_data\n",
    "        )\n",
    "    ORDER BY tutorial_search_clip_clipen_similarity1 DESC \n",
    "    LIMIT 5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc8f19-ab81-496e-860b-dda32b69c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "PRINT IMAGE \n",
    "AS (\n",
    "    SELECT image_path, tutorial_search_clip_clipen_similarity1 \n",
    "    FROM (\n",
    "        SEARCH IMAGE text=\"the feeling when your program finally works\"\n",
    "        USING tutorial_search_clip\n",
    "        AS \n",
    "        SELECT * \n",
    "        FROM unsplash_data\n",
    "        )\n",
    "    ORDER BY tutorial_search_clip_clipen_similarity1 DESC \n",
    "    LIMIT 5\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
