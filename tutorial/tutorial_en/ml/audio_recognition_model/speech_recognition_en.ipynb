{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cfd2a8c-fdfc-4233-abd1-ece097069522",
   "metadata": {},
   "source": [
    "# __Create a speech recognition model to dictate an audio file__\n",
    "\n",
    "**Understanding Speech Recognition Technology**\n",
    "--- \n",
    "ðŸ‘‰ Speech recognition technology, also known as computer speech recognition or speech-to-text transformation, allows programs to process human speech in text format. Recently, it has been used in a wide range of fields, including automobiles, medical fields, and daily life using artificial intelligence speakers or smartphones. Recently, speech recognition technology using machine learning algorithms integrates grammar, syntax, structure, audio and voice signal composition to understand and process speech.\n",
    "\n",
    "\n",
    "**In this tutorial**\n",
    "--- \n",
    "ðŸ‘‰ Librispeech [Panayotov et al. 2015] is one of the most widely used large-scale English speech data in speech recognition research and is the result of the user-participating audiobook project [LibriVox project](https://librivox.org/). About 1,000 hours of recorded audio book data sampled at 16 kHz was processed and created. The target table for the tutorial consists of a path and script for a pre-uploaded voice file. This tutorial aims to convert voice files into text.\n",
    "\n",
    "__Precautions__    \n",
    "> - The audio file format currently supported by ThanoSQL is ''.It's wav, '.flac'.\n",
    "> - A column representing the audio file path and a column representing the text corresponding to the target must exist in the table.\n",
    "> - The base model (Wav2Vec2En) of that speech recognition model uses a GPU. Depending on the size and batch size of the model used, GPU memory may be low. In this case, try using a smaller model or reducing the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5a038e-2951-433a-b5b2-2cc0cf439d2e",
   "metadata": {},
   "source": [
    "## __0. Preparing a dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d0caec",
   "metadata": {},
   "source": [
    "To use the query syntax of ThanoSQL, you must create an API token and run the query below, as mentioned in the [ThanoSQL Workspace](https://docs.thanosql.ai/quick_start/how_to_use_ThanoSQL/#5-thanosql)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb93ef5-7309-4842-b616-f8269965db46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext thanosql\n",
    "%thanosql API_TOKEN=<Issued_API_TOKEN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d86c0-984b-4df7-a46c-b8c2294e4e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "COPY librispeech_train \n",
    "OPTIONS(overwrite =True)\n",
    "FROM \"tutorial_data/librispeech_data/librispeech_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe4e8d3-bbd6-4fcb-92f2-0f6263612af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "COPY librispeech_test \n",
    "OPTIONS(overwrite =True)\n",
    "FROM \"tutorial_data/librispeech_data/librispeech_test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984aefd3",
   "metadata": {},
   "source": [
    "__OPTIONS__ : \n",
    "\n",
    "When __overwrite is true__, the user can create a data table with the same name as the previously created data table.  \n",
    "On the other hand, when __overwrite is False__, the user cannot create a data table with the same name as the previously created data table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e557a156-1075-41df-b19f-ff0812a14b4c",
   "metadata": {},
   "source": [
    "## __1. Check the dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e7fa3-29ac-4d16-8602-87cb62ec45ae",
   "metadata": {},
   "source": [
    "To proceed with this tutorial, we use the librispech_train table stored in the ThanoSQL DB. Run the query statement below to verify the contents of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d801df-54d2-4809-bbed-69b2818e9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "SELECT *\n",
    "FROM librispeech_train\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff864330",
   "metadata": {},
   "source": [
    "__Understanding data__ \n",
    "- audio : location path of the audio file\n",
    "- text : Target value of the corresponding voice (Target, Script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d7b7cc-e411-4e39-b443-1aa287a0adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "PRINT AUDIO \n",
    "AS\n",
    "SELECT audio_path\n",
    "FROM librispeech_train\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3251b51-e9d8-4c5c-9882-46ca1c5d58bd",
   "metadata": {},
   "source": [
    "## __2. Predict Speech Recognition Results Using Pretrained Models__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7bc05-3d37-47e3-b5e9-bb10b5043818",
   "metadata": {},
   "source": [
    "First, let's predict the results with the model we prepared in advance. Running the following query statement allows you to predict results using the tutorial_image_classification model, a pre-trained speech recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef603c7d-ac80-4918-bb9d-d4abe3e07d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "PREDICT USING tutorial_audio_recognition\n",
    "OPTIONS (\n",
    "    audio_col='audio_path',\n",
    "    text_col='text', \n",
    "    epochs=1, \n",
    "    batch_size=8\n",
    "    )\n",
    "AS \n",
    "SELECT * \n",
    "FROM librispeech_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b4137e-75ad-4bea-bda0-0c7fcdb3b72b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## __3. Create a speech recognition model__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1c22c1-1d26-40b6-9216-6a0d5ed49d2a",
   "metadata": {},
   "source": [
    "Create a speech recognition model using the librispech_train dataset that you saw in the previous step. Run the query syntax below to create a model named my_speech_recognition_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb0317-476e-409f-baa3-dc562d924e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "BUILD MODEL my_speech_recognition_model\n",
    "USING Wav2Vec2En\n",
    "OPTIONS (\n",
    "    audio_col='audio_path',  \n",
    "    text_col='text',  \n",
    "    epochs=1,  \n",
    "    batch_size=4,\n",
    "    overwrite= True  \n",
    "    )\n",
    "AS\n",
    "SELECT *\n",
    "FROM librispeech_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e61e174",
   "metadata": {},
   "source": [
    "__Query Details__ \n",
    "- \"__BUILD MODEL__\" Create and train my_speech_recognition_model using the query syntax.\n",
    "- \"__USING__\" The query syntax specifies the use of `Wav2Vec2En` as the base model.\n",
    "- \"__OPTIONS__\" Specifies the options used to create the model through the query syntax.\n",
    "    - \"audio_col\" : Name of the column containing the audio path to be used for learning\n",
    "    - \"text_col\" :  Name of column containing script information for audio\n",
    "    - \"epochs\" : Number of times to learn all learning datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2779faae",
   "metadata": {},
   "source": [
    "__OPTIONS__ : \n",
    "\n",
    "When __overwrite is true__, the user can create a data table with the same name as the previously created data table.  \n",
    "On the other hand, when __overwrite is False__, the user cannot create a data table with the same name as the previously created data table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6402263-ff45-45e8-b221-27c1ff97c556",
   "metadata": {},
   "source": [
    "## __4. Predict speech recognition results using the model you created__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce74d882-0507-42b2-8d7e-d42cf23b0457",
   "metadata": {},
   "source": [
    "Use the speech recognition model you created in the previous step to predict the target (script) of a particular speech (data table not used for learning, librispech_test). After performing the query below, the prediction results are stored in the predicated column and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b828d-962c-4115-a417-59ca2c621e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%thanosql\n",
    "PREDICT USING my_speech_recognition_model\n",
    "OPTIONS (\n",
    "    audio_col='audio_path'\n",
    "    )\n",
    "AS\n",
    "SELECT *\n",
    "FROM librispeech_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d9847",
   "metadata": {},
   "source": [
    "__Query Details__\n",
    "- Use the my_speech_recognition_model created in the previous step with the query syntax \"__PREDICTUSING__\".\n",
    "- Specify the options to use for prediction via the \"__OPTIONS__\" query syntax.\n",
    "    - \"audio_col\" : Name of the column containing the audio path to be used for prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": false,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
